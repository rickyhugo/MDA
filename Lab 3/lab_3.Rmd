---
output:
    pdf_document:
        number_sections: true
header-includes:
    \usepackage{times}
    \usepackage{bm}
    \usepackage[labelfont={bf}, font={it}]{caption}
    \usepackage{amsmath}
    \usepackage{float}
    \floatplacement{figure}{H}
---

\section{Part I: Computer Exercises}

We import some exclusive packages.
```{r, echo=T, results='hide', message=F, warning=F}
library(lattice)
library(ggplot2)
library(caret)
library(SimDesign)
library(MASS)
library(matlib)
library(Rfast)
```

\subsection{General Theory}
In this Home Assignment, we will use the Linear Discriminant Analysis (LDA) as probability model, as presented in Home Assignment 1, Task 2. This model will be compared to Fisher's LDA, which solves the same problem with a non-parametric approach. In short, Fisher's LDA derives the optimal direction of the discriminant, or the weights, by maximizing the distance in mean as well as minimizing the within-class variance. These objectives can be summarized as
$$
\max_{\bm{w}} J(\bm{w}) = \frac{\bm{w}^\intercal \bm{S}_B \bm{w}}{\bm{w}^\intercal \bm{S}_W \bm{w}}
$$
where $\bm{S}_B = (\bm{\bar{x}}_2 - \bm{\bar{x}}_1)(\bm{\bar{x}}_2 - \bm{\bar{x}}_1)^\intercal$ is the between-class variance, and $\bm{S}_W = \sum_{i \in \mathcal{C}_1} (\bm{\bar{x}}_i - \bm{\bar{x}}_1)(\bm{\bar{x}}_i - \bm{\bar{x}}_1)^\intercal + \sum_{i \in \mathcal{C}_2} (\bm{\bar{x}}_i - \bm{\bar{x}}_2)(\bm{\bar{x}}_i - \bm{\bar{x}}_2)^\intercal$ is the within-class variance. It can be shown that the maximum is obtained by letting $\bm{w} = \bm{S}^{-1}_W(\bm{\bar{x}}_2 - \bm{\bar{x}}_1)$. It should also be noted that, in the two-dimensional case, the Fisher's approach yields the same results as the "ordinary", Gaussian approach.

\subsection{Task 1}
\subsubsection{Method}
Firstly, the two sets of bivariate normal variables with corresponding labels, "1" and "2", were generated. Then, these were visualized through scatter plot where the first class was colored red, and the other one green.

\subsubsection{Source Code}
Generate two sets of bivariate normal numbers and visualize these with different colors.


```{r, fig.cap="Visualization of the two sets of simulated normal multivariate numbers."}
set.seed(1337)
n1 = n2 = 150
mu1 = c(0, 0)
mu2 = c(-3, 2)

z1 = SimDesign::rmvnorm(n1, mu1)
lab1 = rep(1, n1)

z2 = SimDesign::rmvnorm(n2, mu2)
lab2 = rep(2, n2)
```


```{r}
plot(
    z1,
    col=2,
    xlim=c(-7, 4),
    ylim = c(-3, 5),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2"),
    pch=20,
    col=c(2, 3),
    bg="white"
)
```



\subsection{Task 2}
\subsubsection{Method}
Consequently, the LDA model was fitted through MLE estimation, which simply corresponds to the sample mean and the pooled covariance matrix, since the simulated variables were uncorrelated. Thus, the decision boundary could be expressed as
$$
\hat{y} = (\Bar{x}_2 - \Bar{x}_1)^\intercal S_{p}^{-1}\bm{x} = \frac{1}{2} (\Bar{x}_1 + \Bar{x}_2)^\intercal S_{p}^{-1} (\Bar{x}_2 - \Bar{x}_1) = \hat{m},
$$
where $S_p$ is the pooled covariance matrix.

\subsubsection{Source Code}
Estimate the LDA parameters by MLE, i.e., computing sample mean and pooled covariance matrix, since our simulated sets of variables are uncorrelated. Then, we compute the corresponding decision boundary and plot it together with the simulated values.
```{r}
mu1_s = colMeans(z1)
mu2_s = colMeans(z2)
S = pooled.cov(rbind(z1, z2), c(lab1, lab2)) # pooled covariance

MU_pos = mu1_s + mu2_s
dim(MU_pos) = c(2, 1)

MU_neg = mu2_s - mu1_s
dim(MU_neg) = c(2, 1)

c = 0.5 * t(MU_pos) %*% inv(S) %*% MU_neg
x = t(MU_neg) %*% inv(S)

k = x[1] / -x[2]
m = c / x[2]
```




```{r, fig.cap="Visualization of the two sets of simulated normal multivariate numbers, along with the MLE decision boundary."}
plot(
    z1,
    col=2,
    xlim=c(-7, 4),
    ylim = c(-3, 5),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
abline(a=m, b=k)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2"),
    pch=20,
    col=c(2, 3),
    bg="white"
)
```



\subsection{Task 3}
\subsubsection{Method}
Further, a LDA was performed by the built-in LDA function. This function computes the direction of Fisher's projection, i.e., $\bm{w}$, as defined in Section 1.1. Then, the corresponding decision boundary was calculated and visualized side-by-side with the boundary based on MLE estimation.

\subsubsection{Conclusion}
As expected, the setting in the two-dimensional case, yields the exact same decision boundary for both the ordinary LDA and Fisher's method.

\subsubsection{Source Code}

Create a neat data frame with the simulated values and their corresponding labels.
```{r}
# stack observations and labels to data frame
Z = rbind(z1, z2)
Y = as.factor(c(lab1, lab2))
dat = as.data.frame(cbind(Z, Y))
```

Fit LDA with built-in package in R.
```{r}
# fit LDA model
mdl = lda(
    Y ~ V1 + V2,
    data=dat
)
```

Consequently, we compute the decision boundary based on the built-in LDA and compare it with the one in Task 2.
```{r}
mu = mdl$means
w = mdl$scaling
mu_vec = colSums(mu)
dim(mu_vec) = c(2, 1)
c = 0.5 * t(w) %*% mu_vec
lda_k = w[1] / -w[2]
lda_m = c / w[2]
```


```{r, echo=FALSE, fig.cap="Visualization of the two sets of simulated normal multivariate numbers, along with the MLE (l.h.s.) and built-in LDA (r.h.s.) decision boundary, respectively."}
par(mfrow=c(1, 2))
plot(
    z1,
    col=2,
    xlim=c(-7, 4),
    ylim = c(-3, 5),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20,
)
points(z2, col=3, pch=20)
abline(a=m, b=k, lty=1, lwd=2)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2"),
    pch=20,
    col=c(2, 3),
    bg="white"
)
legend('topright', 'MLE', lty=1, bg='white')

plot(
    z1,
    col=2,
    xlim=c(-7, 4),
    ylim = c(-3, 5),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
abline(a=lda_m, b=lda_k, lty=2, lwd=2)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2"),
    pch=20,
    col=c(2, 3),
    bg="white"
)
legend('topright', 'lda()', lty=2,  bg='white')
```

The decision boundary for each method presented on the form $y = kx + m$, which should be identical.
```{r}
paste("Task 2 - MLE decision boundary: y = ",
      round(k, 4), "x + ", round(m, 4), sep=""
)
paste("Task 3 - lda() decision boundary: y = ",
      round(lda_k, 4), "x + ", round(lda_m, 4), sep=""
)
```


\subsection{Task 4}
\subsubsection{Method}
We simply followed the nice and clear steps requested in the lab specification!

\subsubsection{Conclusions}
Keeping Figure 7 in mind, it becomes clear that the two methods results in the same separation of the classes. The multiclass LDA model yields two decision boundaries which can be interpreted as majority voting w.r.t. the discriminants, when distinguishing between the classes. The one-versus-one approach, on the other hand, yields three decision boundaries, which combined together results in the same predictions as the LDA (see Figure 7). The combination of the three decision boundaries could also be viewed, in this matter, as majority voting. Since we assume linearly separable classes, these results are not unexpected. Last but not least, it is exciting that the multiclass LDA is equivalent to the one-versus-one approach under these presumptions.

\subsubsection{Source Code}
Initially, we generate a new set of observations with label "3".

```{r}
# generate another set of observations
set.seed(1337)
n3 = 150
mu3 = c(-1, -3)

z3 = SimDesign::rmvnorm(n3, mu3)
lab3 = rep(3, n3)
```


\paragraph{Part 1}
Then, we visualize our three classes, one color for each.
```{r, fig.cap="Visualization of the three sets of simulated normal multivariate numbers."}
plot(
    z1,
    col=2,
    xlim=c(-7, 4),
    ylim = c(-7, 6),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
points(z3, col=4, pch=20)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2", "Class 3"),
    pch=20,
    col=c(2, 3, 4),
    bg="white"
)
```

\paragraph{Part 2}
In this part, we first stack our data into a new data frame, including all three classes.
```{r}
# create data frame for all three classes
Z = rbind(z1, z2, z3)
Y = as.factor(c(lab1, lab2, lab3))
dat3 = as.data.frame(cbind(Z, Y))
```

Consequently, we fit a LDA model, using the built-in function.
```{r}
# fit 3-class LDA
Mod = lda(
    Y ~ V1 + V2,
    data=dat3
)
```

\paragraph{Part 3}
Then, we generate a "prediction grid" as requested.
```{r}
# data frame
x1 = seq(-6, 4, 0.1)
x2 = seq(-6, 6, 0.1)
d = expand.grid(x1, x2)
names(d) = c("V1", "V2")
```

\paragraph{Part 4}
The grid of points were then classified by the LDA model fitted in Part 2, and colored accordingly.
```{r}
d_pred = predict(Mod, d)
```

\paragraph{Part 5}
The separation of the grid, based on the previous LDA classification, is visualized by one color for each class.
```{r}
# extract predicted points and label according to class
d1_idx = d_pred$class == 1
d2_idx = d_pred$class == 2
d3_idx = d_pred$class == 3

d1_pred = d[d1_idx, ]
d2_pred = d[d2_idx, ]
d3_pred = d[d3_idx, ]
```



```{r, echo=FALSE, fig.cap="Visualization of the predictions of the generated grid (in Part 3) based on the classifications in Part 4."}
plot(
    z1,
    col=2,
    xlim=c(-6, 4),
    ylim = c(-6, 6),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
points(z3, col=4, pch=20)

points(
    d1_pred,
    col=2,
    pch=20
)

points(
    d2_pred,
    col=3,
    pch=20
)

points(
    d3_pred,
    col=4,
    pch=20
)

grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2", "Class 3"),
    pch=20,
    col=c(2, 3, 4),
    bg="white"
)
```




Further, the two decision boundaries from the LDA model in Part 2 was visualized, along with the initial observations of the three classes. Solely to get a deeper understanding of the model.
```{r, include=F}
mu5 = Mod$means
w5 = Mod$scaling
mu5_vec = colSums(mu5)
dim(mu5_vec) = c(2, 1)
c5 = (1 / 3) * w5 %*% mu5_vec
```

```{r, include=F}
lda_k5_1 = w5[1, 1] / -w5[1, 2]
lda_m5_1 = c5[1] / w5[1, 2]

lda_k5_2 = w5[2, 1] / -w5[2, 2]
lda_m5_2 = c5[2] / w5[2, 2]
```




```{r, include=F, fig.cap="Visualization of decision boundaries based on the built-in LDA."}
plot(
    z1,
    col=2,
    xlim=c(-6, 4),
    ylim = c(-6, 6),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
points(z3, col=4, pch=20)

abline(a=lda_m5_1, b=lda_k5_1, lty=1)
abline(a=lda_m5_2, b=lda_k5_2, lty=2)
grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2", "Class 3"),
    pch=20,
    col=c(2, 3, 4),
    bg="white"
)
```


\paragraph{Part 6}
We fit another LDA based on MLE as in Task 2, but this time using an one-versus-one approach, which results in three different decision boundaries.
```{r}
mu1_s = colMeans(z1)
mu2_s = colMeans(z2)
mu3_s = colMeans(z3)
S = pooled.cov(rbind(z1, z2, z3), c(lab1, lab2, lab3)) # pooled covariance
```


```{r}
MU_pos = mu1_s + mu2_s
dim(MU_pos) = c(2, 1)

MU_neg = mu2_s - mu1_s
dim(MU_neg) = c(2, 1)

c = (1 / 2) * t(MU_pos) %*% inv(S) %*% MU_neg
x = t(MU_neg) %*% inv(S)

k1 = x[1] / -x[2]
m1 = c / x[2]
```

```{r}
MU_pos = mu1_s + mu3_s
dim(MU_pos) = c(2, 1)

MU_neg = mu3_s - mu1_s
dim(MU_neg) = c(2, 1)

c = (1 / 2) * t(MU_pos) %*% inv(S) %*% MU_neg
x = t(MU_neg) %*% inv(S)

k2 = x[1] / -x[2]
m2 = c / x[2]
```

```{r}
MU_pos = mu2_s + mu3_s
dim(MU_pos) = c(2, 1)

MU_neg = mu3_s - mu2_s
dim(MU_neg) = c(2, 1)

c = (1 / 2) * t(MU_pos) %*% inv(S) %*% MU_neg
x = t(MU_neg) %*% inv(S)

k3 = x[1] / -x[2]
m3 = c / x[2]

```


Lastly, the one-versus-one decision boundaries along with the prediction grid was plotted, in order to understand the relationship between the different approaches, i.e. one-versus-one and multiclass LDA, respectively.


```{r, echo=FALSE, fig.cap="The decision boundaries using the one-versus-one approach (Part 6), along with the predictions of the generated grid (Part 3)."}
plot(
    z1,
    col=2,
    xlim=c(-6, 4),
    ylim = c(-6, 6),
    xlab=expression('x'[1]),
    ylab=expression('x'[2]),
    pch=20
)
points(z2, col=3, pch=20)
points(z3, col=4, pch=20)

points(
    d1_pred,
    col=2,
    pch=20
)

points(
    d2_pred,
    col=3,
    pch=20
)

points(
    d3_pred,
    col=4,
    pch=20
)

abline(a=m1, b=k1, lty=1, lwd=2)
abline(a=m2, b=k2, lty=2, lwd=2)
abline(a=m3, b=k3, lty=3, lwd=2)

grid()
legend(
    "topleft",
    legend=c("Class 1", "Class 2", "Class 3"),
    pch=20,
    col=c(2, 3, 4),
    bg="white"
)
```